---
phase: 02-pose-detection
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/hooks/usePoseDetection.ts, src/components/PoseCanvas.tsx, src/components/CameraPreview.tsx, src/types/pose.ts]
autonomous: true
---

<objective>
Integrate MediaPipe Pose detection and draw 33-point skeleton overlay on live video.

Purpose: Enable real-time body landmark tracking as foundation for form feedback.
Output: Working skeleton overlay that tracks body movements in real-time.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-02-SUMMARY.md

@src/hooks/useCamera.ts
@src/components/CameraPreview.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install MediaPipe and create pose types</name>
  <files>package.json, src/types/pose.ts</files>
  <action>
    1. Install MediaPipe Pose: `npm install @mediapipe/pose @mediapipe/camera_utils @mediapipe/drawing_utils`
    2. Create src/types/pose.ts with:
       - `Landmark` interface: { x: number, y: number, z: number, visibility: number }
       - `PoseResults` interface: { poseLandmarks: Landmark[] | null }
       - Export landmark index constants for key points (e.g., LEFT_SHOULDER = 11, RIGHT_HIP = 24)
       - Reference: MediaPipe Pose has 33 landmarks (0-32)

    Key landmarks to export as constants:
    - NOSE = 0, LEFT_EYE = 2, RIGHT_EYE = 5
    - LEFT_SHOULDER = 11, RIGHT_SHOULDER = 12
    - LEFT_ELBOW = 13, RIGHT_ELBOW = 14
    - LEFT_WRIST = 15, RIGHT_WRIST = 16
    - LEFT_HIP = 23, RIGHT_HIP = 24
    - LEFT_KNEE = 25, RIGHT_KNEE = 26
    - LEFT_ANKLE = 27, RIGHT_ANKLE = 28
  </action>
  <verify>npm install succeeds, TypeScript compiles (npm run build)</verify>
  <done>MediaPipe installed, pose types defined with landmark constants</done>
</task>

<task type="auto">
  <name>Task 2: Create usePoseDetection hook</name>
  <files>src/hooks/usePoseDetection.ts</files>
  <action>
    Create custom hook that:
    1. Accepts videoRef (RefObject<HTMLVideoElement>) as parameter
    2. Initializes MediaPipe Pose with options:
       - modelComplexity: 1 (balanced accuracy/performance)
       - smoothLandmarks: true
       - minDetectionConfidence: 0.5
       - minTrackingConfidence: 0.5
    3. Sets up onResults callback to update landmarks state
    4. Uses requestAnimationFrame loop to send frames from video to detector
    5. Cleans up on unmount (close Pose instance)
    6. Returns: { landmarks: Landmark[] | null, isDetecting: boolean }

    Important implementation details:
    - Use useRef for Pose instance to avoid re-initialization
    - Check video.readyState >= 2 before sending frames
    - Handle facingMode mirroring: flip x coordinates when facingMode='user'
    - Accept facingMode as optional second parameter for coordinate flipping
  </action>
  <verify>Hook compiles without TypeScript errors</verify>
  <done>usePoseDetection hook returns landmarks array from video frames</done>
</task>

<task type="auto">
  <name>Task 3: Create PoseCanvas component and integrate skeleton overlay</name>
  <files>src/components/PoseCanvas.tsx, src/components/CameraPreview.tsx</files>
  <action>
    Create PoseCanvas component:
    1. Accepts landmarks array, width, height, facingMode props
    2. Renders canvas element positioned absolutely over video
    3. Uses useEffect to draw on canvas when landmarks change
    4. Draw skeleton using MediaPipe connections:
       - Lines between connected landmarks (shoulders, spine, arms, legs)
       - Circles at each landmark point
       - Use bright color (cyan/lime) for visibility against varied backgrounds
    5. Clear canvas each frame before redrawing
    6. Scale landmark coordinates (0-1) to canvas dimensions

    Update CameraPreview:
    1. Import and use usePoseDetection hook, passing videoRef and facingMode
    2. Get video dimensions using useRef + resize observer or fixed dimensions
    3. Overlay PoseCanvas component on top of video element
    4. Pass landmarks, dimensions, and facingMode to PoseCanvas
    5. Ensure canvas matches video dimensions (use same responsive classes)

    Skeleton connections to draw (MediaPipe POSE_CONNECTIONS):
    - Face: nose to eyes
    - Torso: shoulders to hips (forming box)
    - Arms: shoulder → elbow → wrist (both sides)
    - Legs: hip → knee → ankle (both sides)
    - Spine: midpoint(shoulders) to midpoint(hips)
  </action>
  <verify>npm run dev shows skeleton overlay tracking body movements in real-time</verify>
  <done>33-point skeleton renders over live video, tracks user movements</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds without errors
- [ ] Skeleton overlay appears on live video
- [ ] All 33 landmarks visible when full body in frame
- [ ] Landmarks track smoothly as user moves
- [ ] Overlay correctly mirrors for front camera
- [ ] No console errors during detection
</verification>

<success_criteria>
- All tasks completed
- MediaPipe Pose detecting landmarks in real-time
- Skeleton overlay draws correctly over video
- Works on both mobile and desktop
- Satisfies POSE-01: 33 body landmarks displayed
</success_criteria>

<output>
After completion, create `.planning/phases/02-pose-detection/02-01-SUMMARY.md`
</output>
